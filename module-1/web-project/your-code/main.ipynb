{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re, string, unicodedata\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from textblob import TextBlob\n",
    "\n",
    "#import contractions\n",
    "#import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "violence_words = set([\"asesinato\", \"asesino\", \"asesina\", \"matar\", \"mata\", \"mato\", \"apuñalar\",\"apuñalo\",\"pega\",\"pego\",\"apalizo\",\"golpea\",\"golpeo\",\"agredio\",\"agrede\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mato', 'apalizo', 'pego', 'golpea', 'mata', 'asesino', 'agredio', 'asesina', 'apuñalo', 'apuñalar', 'pega', 'agrede', 'matar', 'asesinato', 'golpeo'}\n"
     ]
    }
   ],
   "source": [
    "print(violence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'otra', 'tengan', 'tuviese', 'nuestra', 'estáis', 'estaban', 'tuyo', 'hasta', 'vosostros', 'he', 'estabas', 'tuviera', 'él', 'poco', 'fuiste', 'seáis', 'tenía', 'uno', 'vuestro', 'tenéis', 'estar', 'vuestros', 'estarás', 'desde', 'hayan', 'muchos', 'nuestros', 'habremos', 'os', 'otros', 'entre', 'sí', 'otras', 'nuestro', 'y', 'mi', 'estuvieran', 'habidos', 'una', 'fuesen', 'tenido', 'tuviste', 'habían', 'serás', 'seremos', 'siente', 'eso', 'habría', 'esto', 'nosotras', 'hubierais', 'están', 'teníais', 'han', 'del', 'estuve', 'cuando', 'esos', 'haya', 'tuvieran', 'fueron', 'tienes', 'fueras', 'mucho', 'algunas', 'todo', 'esté', 'esas', 'tuviésemos', 'estuvo', 'fuésemos', 'e', 'sentida', 'ya', 'ante', 'tus', 'fueran', 'hay', 'hube', 'algo', 'tienen', 'o', 'sentid', 'fuéramos', 'somos', 'tendría', 'la', 'hayas', 'serías', 'sus', 'era', 'para', 'durante', 'tuviesen', 'estuviera', 'estado', 'vosostras', 'tendríamos', 'has', 'estaría', 'sea', 'éramos', 'también', 'hayamos', 'estas', 'estoy', 'en', 'habéis', 'tenían', 'nosotros', 'tengamos', 'teniendo', 'seríamos', 'tuviéramos', 'estuviese', 'había', 'nuestras', 'ellas', 'tuyos', 'tendrían', 'hubiéramos', 'serían', 'fuisteis', 'nos', 'más', 'estarán', 'te', 'tenías', 'habrás', 'de', 'ti', 'tuvieron', 'porque', 'estaríais', 'fuimos', 'donde', 'tenga', 'antes', 'tengáis', 'nada', 'eres', 'tuvo', 'estuvimos', 'míos', 'tuvisteis', 'estarían', 'hubieseis', 'teníamos', 'hemos', 'vuestra', 'habrías', 'tú', 'estad', 'hubo', 'qué', 'estuviésemos', 'hubiesen', 'tuvieseis', 'esta', 'cual', 'ha', 'estando', 'estaréis', 'habíamos', 'hubieron', 'tendrá', 'suyo', 'quien', 'vuestras', 'los', 'mías', 'sentido', 'estuvieseis', 'seamos', 'ellos', 'tu', 'seas', 'mía', 'ese', 'soy', 'erais', 'estuvieses', 'tendrán', 'tendrás', 'seréis', 'un', 'este', 'serán', 'fui', 'hubieran', 'estos', 'habrían', 'hubieras', 'tenida', 'habríamos', 'tengo', 'tanto', 'estará', 'estarías', 'estén', 'estuvisteis', 'fueseis', 'sentidos', 'ella', 'estados', 'estuviéramos', 'estaremos', 'estés', 'estéis', 'habiendo', 'suyos', 'tuve', 'fuerais', 'estamos', 'estadas', 'habido', 'habidas', 'tenidos', 'a', 'tuvierais', 'tiene', 'por', 'tendremos', 'seré', 'se', 'hubiste', 'estaba', 'habíais', 'sintiendo', 'tuvimos', 'todos', 'fueses', 'mío', 'muy', 'eras', 'tendríais', 'tendré', 'otro', 'estuviste', 'tuyas', 'estuvieron', 'fuera', 'tened', 'fue', 'sentidas', 'estás', 'estuviesen', 'fuese', 'sois', 'no', 'habrá', 'estada', 'hubiésemos', 'mí', 'tenidas', 'suya', 'estábamos', 'estuvieras', 'habías', 'estuvierais', 'habré', 'hubisteis', 'seríais', 'al', 'será', 'algunos', 'sin', 'está', 'su', 'el', 'estaré', 'contra', 'hubimos', 'estemos', 'sean', 'tenemos', 'esa', 'hubieses', 'lo', 'sobre', 'suyas', 'tuya', 'les', 'son', 'es', 'habrán', 'las', 'tuvieras', 'mis', 'me', 'estaríamos', 'yo', 'pero', 'ni', 'que', 'tuvieses', 'con', 'eran', 'unos', 'habida', 'tendrías', 'tengas', 'hubiera', 'quienes', 'sería', 'hayáis', 'como', 'tendréis', 'habríais', 'le', 'habréis', 'hubiese', 'estabais'}\n"
     ]
    }
   ],
   "source": [
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MADRID_URL = \"https://www.madridiario.es/sucesos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\") #parser para que no se queje\n",
    "    return soup\n",
    "\n",
    "soup = get_soup(MADRID_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headers</th>\n",
       "      <th>datePublication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agresión a un cerrajero cuando cambiaba la cer...</td>\n",
       "      <td>2019-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muere un mecánico al caerle un coche encima en...</td>\n",
       "      <td>2019-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vuelca con un coche robado en su huída de la G...</td>\n",
       "      <td>2019-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un incendio destruye una nave industrial en Hu...</td>\n",
       "      <td>2019-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un niño de 8 años, grave tras un ahogamiento e...</td>\n",
       "      <td>2019-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Una anciana, crítica al quedar atrapada en su ...</td>\n",
       "      <td>2019-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>La auxiliar de enfermería de Alcalá, culpable ...</td>\n",
       "      <td>2019-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Desarticulada una organización de trata de tra...</td>\n",
       "      <td>2019-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Detenidos por acordar el asesinato de un hombr...</td>\n",
       "      <td>2019-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Libertad para el presunto pederasta que simula...</td>\n",
       "      <td>2019-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Agrede a su pareja en Hortaleza mordiéndole el...</td>\n",
       "      <td>2019-06-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              headers datePublication\n",
       "0   Agresión a un cerrajero cuando cambiaba la cer...      2019-06-27\n",
       "1   Muere un mecánico al caerle un coche encima en...      2019-06-27\n",
       "2   Vuelca con un coche robado en su huída de la G...      2019-06-27\n",
       "3   Un incendio destruye una nave industrial en Hu...      2019-06-27\n",
       "4   Un niño de 8 años, grave tras un ahogamiento e...      2019-06-26\n",
       "5   Una anciana, crítica al quedar atrapada en su ...      2019-06-26\n",
       "6   La auxiliar de enfermería de Alcalá, culpable ...      2019-06-26\n",
       "7   Desarticulada una organización de trata de tra...      2019-06-26\n",
       "8   Detenidos por acordar el asesinato de un hombr...      2019-06-26\n",
       "9   Libertad para el presunto pederasta que simula...      2019-06-26\n",
       "10  Agrede a su pareja en Hortaleza mordiéndole el...      2019-06-25"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get headers and its publication date\n",
    "def get_headers(soup):\n",
    "    headers = []\n",
    "    date = []\n",
    "    \n",
    "    container = soup.find_all('div',{'class','n1 fueraNoticia'})\n",
    "    for item in container:\n",
    "        headers.append(item.find('h2',{'class','titulo'}).text)\n",
    "        date.append(item.find('meta',itemprop='datePublished').get('content')[:10])\n",
    "\n",
    "    if len(headers) == len(date):\n",
    "        return pd.DataFrame({\"headers\": headers, \"datePublication\": date})\n",
    "    else:\n",
    "        return -1 #exit with error because there is news with date\n",
    "\n",
    "        \n",
    "dfnews_raw = get_headers(soup)\n",
    "dfnews_raw.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headers</th>\n",
       "      <th>datePublication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agresión a un cerrajero cuando cambiaba la cer...</td>\n",
       "      <td>2019-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muere un mecánico al caerle un coche encima en...</td>\n",
       "      <td>2019-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vuelca con un coche robado en su huída de la G...</td>\n",
       "      <td>2019-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un incendio destruye una nave industrial en Hu...</td>\n",
       "      <td>2019-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un niño de 8 años, grave tras un ahogamiento e...</td>\n",
       "      <td>2019-06-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headers datePublication\n",
       "0  Agresión a un cerrajero cuando cambiaba la cer...      2019-06-27\n",
       "1  Muere un mecánico al caerle un coche encima en...      2019-06-27\n",
       "2  Vuelca con un coche robado en su huída de la G...      2019-06-27\n",
       "3  Un incendio destruye una nave industrial en Hu...      2019-06-27\n",
       "4  Un niño de 8 años, grave tras un ahogamiento e...      2019-06-26"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of lower-cases\n",
    "def tolower (s):\n",
    "    return s.lower()\n",
    "\n",
    "dfnews_raw['headers'].apply(tolower)\n",
    "dfnews_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Agresion a un cerrajero cuando cambiaba la cer...\n",
      "1    Muere un mecanico al caerle un coche encima en...\n",
      "2    Vuelca con un coche robado en su huida de la G...\n",
      "3    Un incendio destruye una nave industrial en Hu...\n",
      "4    Un niño de 8 años, grave tras un ahogamiento e...\n",
      "5    Una anciana, critica al quedar atrapada en su ...\n",
      "Name: headers, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get rid of accents\n",
    "def clean_accent(s):\n",
    "    return s.replace('á','a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u')\n",
    "\n",
    "news_raw3['headers'] = dfnews_raw.headers.apply(clean_accent)\n",
    "pprint(news_raw3[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of accents\n",
    "def clean_accent(work_list):\n",
    "    return [row.replace('á','a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u') for row in work_list]\n",
    "\n",
    "news_raw2 = clean_accent(news_raw)\n",
    "pprint(news_raw2[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of commas\n",
    "def clean_any_character(work_list, character):\n",
    "    return [row.replace(character,'') for row in work_list]\n",
    "\n",
    "news_raw3 = clean_any_character(news_raw2, ',')\n",
    "pprint(news_raw3[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of quotes\n",
    "def clean_any_character(work_list, character):\n",
    "    return [row.replace(character,'') for row in work_list]\n",
    "\n",
    "news_raw4 = clean_any_character(news_raw3, '\\\"')\n",
    "pprint(news_raw4[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of '\n",
    "def clean_any_character(work_list, character):\n",
    "    return [row.replace(character,'') for row in work_list]\n",
    "\n",
    "news_raw5 = clean_any_character(news_raw4, '\\'')\n",
    "pprint(news_raw5[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of '\n",
    "def clean_any_character(work_list, character):\n",
    "    return [row.replace(character,'') for row in work_list]\n",
    "\n",
    "news_raw6 = clean_any_character(news_raw5, ':')\n",
    "pprint(news_raw6[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each word / Tokenizar\n",
    "def token(work_list):\n",
    "    return [nltk.word_tokenize(row) for row in work_list]\n",
    "\n",
    "token_news = token(news_raw6)\n",
    "print(token_news[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stopwords(list_tokens, stp_words):\n",
    "    [row.remove(item) for row in list_tokens for item in row if item in stp_words]\n",
    "    return list_tokens\n",
    "    \n",
    "cleaned_data = clean_stopwords(token_news, stopWords)\n",
    "pprint(cleaned_data[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#new = new + \" enfado\"\n",
    "new = [' '.join(item) for item in cleaned_data]\n",
    "\n",
    "\n",
    "for item in new:\n",
    "    if TextBlob(item).detect_language() == 'es':\n",
    "        print(\"1\")\n",
    "        analysis = TextBlob(str(item))\n",
    "        item = analysis.translate(to='spanish')\n",
    "        print(item)\n",
    "    analysis = TextBlob(str(item))\n",
    "    print(analysis.sentiment,\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "new = ' '.join(cleaned_data[1])\n",
    "print(new)\n",
    "print(TextBlob(new).detect_language())\n",
    "analysis = TextBlob(str(new))\n",
    "print(analysis.sentiment,\"\\n\")\n",
    "\n",
    "\n",
    "new = ' '.join(cleaned_data[2])\n",
    "print(new)\n",
    "print(TextBlob(new).detect_language())\n",
    "analysis = TextBlob(str(new))\n",
    "print(analysis.sentiment,\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "new2 = analysis.translate(to='spanish')\n",
    "analysis = TextBlob(str(new2))\n",
    "print(analysis.sentiment)\n",
    "print(analysis.tags)\n",
    "#print(analysis.translate(to='spanish'))\n",
    "print(dir(analysis))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_item(list_tokens):\n",
    "    for index, row in enumerate(list_tokens):\n",
    "        list_tokens.insert(index+1, [])\n",
    "        list_tokens.insert(index+1, [])\n",
    "    return list_tokens\n",
    " \n",
    "print(new_item(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violence_at_row(list_tokens,set_list):\n",
    "    for row in list_tokens:\n",
    "        for word in row:\n",
    "            if word in violence_words:\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in cleaned_data:\n",
    "    if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
